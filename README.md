## carpeta papers-referencia contenido

He revisado los cuatro papers que me compartiste y aqu√≠ te explico qu√© hace cada uno:

---

### 1. **IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning (ACL 2024)**

Este paper **presenta un benchmark llamado IL-TUR** para procesamiento de lenguaje natural en el dominio legal de la India.

* Contiene **8 tareas** en ingl√©s y 9 lenguas indias: reconocimiento de entidades legales (L-NER), predicci√≥n de roles ret√≥ricos, predicci√≥n de juicios con explicaci√≥n, predicci√≥n de fianza, identificaci√≥n de estatutos legales, recuperaci√≥n de casos previos, resumen de documentos legales y traducci√≥n autom√°tica legal.
* Su objetivo es **estandarizar datasets y m√©tricas**, ofrecer **baselines (incluyendo LLMs)** y un **leaderboard p√∫blico**.
* La motivaci√≥n principal: los documentos legales en India son largos, multiling√ºes, poco estructurados, y su an√°lisis manual contribuye al enorme atraso judicial (m√°s de 40 millones de casos).
* Aporta un **recurso de referencia para comparar modelos legales-NLP**.

---

### 2. **A Comprehensive Analysis of Indian Legal Documents Summarization Techniques (SN Computer Science, 2023)**

Este trabajo es un **art√≠culo de revisi√≥n y experimentaci√≥n comparativa** sobre t√©cnicas de resumen autom√°tico de documentos legales indios.

* Analiza **7 modelos de resumen**: BART, LexRank, TextRank, Luhn, LSA, Legal Pegasus y Longformer.
* Usan juicios obtenidos de **Indian Kanoon**.
* Concluyen que **Legal Pegasus** obtiene mejor rendimiento que los dem√°s en la tarea de resumen legal.
* Explica la diferencia entre m√©todos **extractivos** (seleccionan oraciones originales) y **abstractive** (generan nuevo texto).
* Aporta un **estado del arte comparativo** que muestra fortalezas y debilidades de cada t√©cnica en el contexto legal indio.

---

### 3. **Legal Case Document Summarization: Extractive and Abstractive Methods and their Evaluation (AACL 2022)**„Äê10:2022.aacl-main.77.pdf„Äë

Este paper **estudia sistem√°ticamente c√≥mo rinden distintos m√©todos de resumen (extractivos vs. abstractive)** en documentos legales.

* Crean **tres datasets**:

  * **IN-Abs**: 7,130 juicios de la Corte Suprema India con res√∫menes ‚Äúheadnotes‚Äù.
  * **IN-Ext**: 50 documentos con res√∫menes extractivos segmentados por roles ret√≥ricos (hechos, argumentos, precedentes, etc.).
  * **UK-Abs**: 793 juicios del Reino Unido con res√∫menes oficiales.
* Eval√∫an m√©todos extractivos (LexRank, SummaRunner, BERTSum), abstractive (BART, Pegasus, Longformer) y espec√≠ficos para el dominio legal (CaseSummarizer, Gist).
* Hallazgos:

  * **Los m√©todos generales a veces superan a los espec√≠ficos del dominio**.
  * Para textos largos, **estrategias de chunking** funcionan mejor que usar modelos abstractive directamente.
  * **Los abogados prefieren res√∫menes extractivos** aunque los autom√°ticos den mejores m√©tricas ROUGE.
* Aporta datasets y un **an√°lisis profundo de qu√© funciona mejor en documentos legales largos**.

---

### 4. **MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments (EMNLP 2023)**„Äê11:2023.emnlp-main.321.pdf„Äë

Este paper introduce **MILDSum**, el primer dataset de res√∫menes legales biling√ºes (ingl√©s‚Äìhindi).

* Consta de **3,122 juicios de cortes indias** en ingl√©s, con res√∫menes en ingl√©s e hindi elaborados por abogados.
* Fuente: art√≠culos de **LiveLaw**, sitio legal que publica res√∫menes profesionales en ambos idiomas.
* Proponen dos enfoques:

  1. **Summarize-then-translate** (resumir en ingl√©s y luego traducir al hindi).
  2. **Cross-lingual summarization directa** (generar resumen en hindi a partir del documento en ingl√©s).
* Resultados: la estrategia **resumir y luego traducir rinde mejor** que el resumen directo biling√ºe, aunque introduce errores de traducci√≥n.
* El dataset tambi√©n sirve para **entrenar/traducir en el dominio legal indio**.
* Contribuci√≥n: **primer recurso multiling√ºe de res√∫menes legales en India**, que busca mejorar el acceso a la justicia en lenguas locales.

---

üìå En resumen:

* **IL-TUR (2024)** ‚Üí Benchmark amplio para m√∫ltiples tareas legales (no solo resumen).
* **Comprehensive Analysis (2023)** ‚Üí Revisi√≥n y comparaci√≥n de modelos de resumen en India.
* **Legal Case Summarization (2022)** ‚Üí Estudio sistem√°tico de extractivo vs. abstractive con nuevos datasets.
* **MILDSum (2023)** ‚Üí Primer dataset biling√ºe (ingl√©s‚Äìhindi) para res√∫menes legales indios.



## üîπ 1. C√≥mo se conectan los papers con el Shared Task

* **IL-TUR (ACL 2024)** ‚Üí Define benchmarks legales amplios (incluyendo summarization). Inspira a crear datasets estandarizados como **InLSum** para este shared task.
* **Legal Case Summarization (AACL 2022)** ‚Üí Demuestra qu√© m√©todos funcionan mejor en juicios indios largos (extractivo, chunking, supervisado vs. no supervisado). Esto orienta las **estrategias de los equipos** en la competencia.
* **Comprehensive Analysis (SN Computer Science 2023)** ‚Üí Compara modelos (BART, Pegasus, Longformer) y concluye que **Legal Pegasus rinde mejor**. Esto sirve como **baseline esperado** para los participantes.
* **MILDSum (EMNLP 2023)** ‚Üí Introduce el multiling√ºe (ingl√©s‚Äìhindi). Aunque el shared task es solo en ingl√©s, marca la **evoluci√≥n futura hacia multiling√ºe**.

üëâ En conjunto, los papers muestran la progresi√≥n natural: **de benchmarks ‚Üí comparaci√≥n de t√©cnicas ‚Üí evaluaci√≥n sistem√°tica ‚Üí multiling√ºe**, y este shared task se ubica justo en esa l√≠nea, pero focalizado en **resumen abstractive de juicios indios en ingl√©s**.

---

## üîπ 2. Flujo del Shared Task (L-SUMM)

1. **Tarea**

   * Generar res√∫menes abstractive (~500 palabras) de juicios indios en ingl√©s.

2. **Dataset (InLSum)**

   * 3 splits:

     * **Train**: 1200 documentos.
     * **Validation**: 200 documentos (para tuning y evaluaci√≥n intermedia).
     * **Test**: 400 documentos (para evaluaci√≥n final).
   * Formato: JSONL con archivos de juicios (`<split>_judg.jsonl`) y res√∫menes de referencia (`<split>_ref_summary.jsonl`).

3. **Fases de la competencia**

   * **Fase 1: Training & Validation** ‚Üí entrenar modelos en train y predecir summaries para validation.
   * **Fase 2: Testing** ‚Üí predecir summaries en el test (oculto).

4. **Entrega**

   * Archivo ZIP con un JSONL (`answer.jsonl`) que tenga `ID` y `Summary`.
   * Ejemplo:

     ```json
     {"ID": "id_100", "Summary": "The Delhi High Court recently said that it hopes and ..."}
     ```

5. **Evaluaci√≥n**

   * **M√©tricas**: ROUGE-2, ROUGE-L, BLEU (escaladas a 0‚Äì100).
   * **Ranking**: combinaci√≥n de las tres.

---

## üîπ 3. Conexi√≥n con m√©tricas (ejemplo intuitivo)

* **ROUGE** ‚Üí hecho para **summarization**, mide **recall** (cu√°nto del contenido de referencia aparece en el resumen generado).
* **BLEU** ‚Üí hecho para **translation**, mide **precision** (cu√°ntos n-grams del resumen generado coinciden con la referencia).
* **ROUGE-L** ‚Üí usa subsecuencia m√°s larga com√∫n, combina precision + recall.

Ejemplo simplificado:

* Referencia: *‚ÄúThe way to make people trustworthy is to trust them.‚Äù*
* Hip√≥tesis: *‚ÄúTo make people trustworthy, you need to trust them.‚Äù*
* Ambos miden solapamiento de palabras/frases, pero BLEU lo hace desde precisi√≥n y ROUGE desde recall.

---

## üîπ 4. Sugerencia de c√≥mo presentar

* **Intro** ‚Üí Explicar que el Shared Task sigue la l√≠nea de avances previos (los papers).
* **Dataset & Fases** ‚Üí Mostrar el esquema InLSum (train/val/test).
* **M√©tricas** ‚Üí Explicar BLEU vs ROUGE con un ejemplo sencillo.
* **Conexi√≥n** ‚Üí Mostrar que los resultados esperados (por ejemplo, que Pegasus o BART rinden bien) ya tienen base en papers previos.